{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2887d3d7-3f7e-409f-9541-3b967dae4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import glob\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import folium \n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5bf062-d357-4695-af65-8c0ea0a825f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read UK data\n",
    "\n",
    "def process_MOPAC_csv_files(file_path):\n",
    "    # Read the CSV file\n",
    "   \n",
    "    df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2387fb6-d7e7-4dad-8bf1-72c02d528536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\temp\\MOPAC\\2022-01-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-02-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-03-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-04-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-05-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-06-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-07-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-10-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-11-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2022-12-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-01-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-02-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-03-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-04-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-05-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-06-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-07-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-08-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-09-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-10-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-11-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2023-12-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2024-01-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2024-02-city-of-london-street.csv\n",
      "\\temp\\MOPAC\\2024-03-city-of-london-street.csv\n"
     ]
    }
   ],
   "source": [
    "# Street crimes all of London\n",
    "\n",
    "b = True\n",
    "for file_path in glob.glob('\\\\temp\\\\MOPAC\\\\*london*street.csv' ): \n",
    "    #if point_year in range(start_year, end_year+1):\n",
    "    #files_to_process = f\"\\\\Users\\\\Casey\\\\Dash\\\\JupyNote\\\\data\\\\Stats19\\\\collision_{point_year}.csv\"\n",
    "    #if file_path == files_to_process: \n",
    "    if file_path != '':\n",
    "        print(file_path) \n",
    "        _df_street = process_MOPAC_csv_files(file_path)\n",
    "        if b == True:\n",
    "            df_street = _df_street.copy()\n",
    "            b = False    \n",
    "        else:\n",
    "            df_street= pd.concat([df_street, _df_street], ignore_index=True)\n",
    "        \n",
    "df_street.to_csv('\\\\Temp\\\\MOPAC\\\\output\\\\rh_street.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec52f9d-1810-4eca-91fd-2e4a19a7280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\temp\\MOPAC\\2020-04-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-05-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-06-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-07-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-08-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-09-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-10-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-11-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2020-12-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2021-01-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2021-02-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2021-03-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2021-04-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-01-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-02-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-03-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-04-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-05-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-06-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-07-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-10-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-11-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2022-12-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-01-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-02-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-03-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-04-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-05-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-06-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-07-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-08-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-09-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-10-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-11-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2023-12-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2024-01-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2024-02-city-of-london-stop-and-search.csv\n",
      "\\temp\\MOPAC\\2024-03-city-of-london-stop-and-search.csv\n"
     ]
    }
   ],
   "source": [
    "# Street crimes all of Stop-and-search London\n",
    "\n",
    "b = True\n",
    "for file_path in glob.glob('\\\\temp\\\\MOPAC\\\\*london*search.csv' ): \n",
    "  \n",
    "    if file_path != '':\n",
    "        print(file_path) \n",
    "        _df_search = process_MOPAC_csv_files(file_path)\n",
    "        if b == True:\n",
    "            df_search = _df_search.copy()\n",
    "            b = False    \n",
    "        else:\n",
    "            df_search= pd.concat([df_search, _df_search], ignore_index=True)\n",
    "        \n",
    "df_search.to_csv('\\\\Temp\\\\MOPAC\\\\output\\\\rh_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4b5a15-f452-49d8-b5be-d6821a8b8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = {\n",
    "    'Barking and Dagenham': {'lat_min': 51.52, 'lat_max': 51.58, 'lon_min': 0.10, 'lon_max': 0.20},\n",
    "    'Barnet': {'lat_min': 51.55, 'lat_max': 51.65, 'lon_min': -0.27, 'lon_max': -0.13},\n",
    "    'Bexley': {'lat_min': 51.43, 'lat_max': 51.50, 'lon_min': 0.10, 'lon_max': 0.20},\n",
    "    'Brent': {'lat_min': 51.53, 'lat_max': 51.59, 'lon_min': -0.30, 'lon_max': -0.21},\n",
    "    'Bromley': {'lat_min': 51.35, 'lat_max': 51.43, 'lon_min': 0.00, 'lon_max': 0.10},\n",
    "    'Camden': {'lat_min': 51.52, 'lat_max': 51.56, 'lon_min': -0.18, 'lon_max': -0.13},\n",
    "    'Croydon': {'lat_min': 51.33, 'lat_max': 51.39, 'lon_min': -0.12, 'lon_max': -0.04},\n",
    "    'Ealing': {'lat_min': 51.49, 'lat_max': 51.55, 'lon_min': -0.32, 'lon_max': -0.24},\n",
    "    'Enfield': {'lat_min': 51.62, 'lat_max': 51.67, 'lon_min': -0.13, 'lon_max': -0.03},\n",
    "    'Greenwich': {'lat_min': 51.44, 'lat_max': 51.51, 'lon_min': 0.00, 'lon_max': 0.05},\n",
    "    'Hackney': {'lat_min': 51.53, 'lat_max': 51.57, 'lon_min': -0.08, 'lon_max': -0.03},\n",
    "    'Hammersmith and Fulham': {'lat_min': 51.47, 'lat_max': 51.51, 'lon_min': -0.24, 'lon_max': -0.18},\n",
    "    'Haringey': {'lat_min': 51.57, 'lat_max': 51.60, 'lon_min': -0.13, 'lon_max': -0.06},\n",
    "    'Harrow': {'lat_min': 51.55, 'lat_max': 51.60, 'lon_min': -0.37, 'lon_max': -0.30},\n",
    "    'Havering': {'lat_min': 51.54, 'lat_max': 51.60, 'lon_min': 0.15, 'lon_max': 0.25},\n",
    "    'Hillingdon': {'lat_min': 51.50, 'lat_max': 51.56, 'lon_min': -0.49, 'lon_max': -0.38},\n",
    "    'Hounslow': {'lat_min': 51.45, 'lat_max': 51.50, 'lon_min': -0.37, 'lon_max': -0.25},\n",
    "    'Islington': {'lat_min': 51.52, 'lat_max': 51.56, 'lon_min': -0.13, 'lon_max': -0.09},\n",
    "    'Kensington and Chelsea': {'lat_min': 51.48, 'lat_max': 51.52, 'lon_min': -0.22, 'lon_max': -0.16},\n",
    "    'Kingston upon Thames': {'lat_min': 51.38, 'lat_max': 51.42, 'lon_min': -0.32, 'lon_max': -0.25},\n",
    "    'Lambeth': {'lat_min': 51.45, 'lat_max': 51.50, 'lon_min': -0.13, 'lon_max': -0.10},\n",
    "    'Lewisham': {'lat_min': 51.43, 'lat_max': 51.47, 'lon_min': -0.04, 'lon_max': 0.03},\n",
    "    'Merton': {'lat_min': 51.39, 'lat_max': 51.43, 'lon_min': -0.23, 'lon_max': -0.16},\n",
    "    'Newham': {'lat_min': 51.50, 'lat_max': 51.55, 'lon_min': 0.00, 'lon_max': 0.06},\n",
    "    'Redbridge': {'lat_min': 51.55, 'lat_max': 51.60, 'lon_min': 0.05, 'lon_max': 0.10},\n",
    "    'Richmond upon Thames': {'lat_min': 51.43, 'lat_max': 51.49, 'lon_min': -0.33, 'lon_max': -0.26},\n",
    "    'Southwark': {'lat_min': 51.47, 'lat_max': 51.51, 'lon_min': -0.09, 'lon_max': -0.03},\n",
    "    'Sutton': {'lat_min': 51.34, 'lat_max': 51.37, 'lon_min': -0.21, 'lon_max': -0.15},\n",
    "    'Tower Hamlets': {'lat_min': 51.50, 'lat_max': 51.53, 'lon_min': -0.06, 'lon_max': 0.00},\n",
    "    'Waltham Forest': {'lat_min': 51.57, 'lat_max': 51.61, 'lon_min': -0.04, 'lon_max': 0.03},\n",
    "    'Wandsworth': {'lat_min': 51.43, 'lat_max': 51.47, 'lon_min': -0.21, 'lon_max': -0.15},\n",
    "    'Westminster': {'lat_min': 51.48, 'lat_max': 51.53, 'lon_min': -0.19, 'lon_max': -0.10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9e32ae-0e38-4552-86ac-7d2101242a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borough(lat, lon, boroughs):\n",
    "    for borough, bounds in boroughs.items():\n",
    "        if bounds['lat_min'] <= lat <= bounds['lat_max'] and bounds['lon_min'] <= lon <= bounds['lon_max']:\n",
    "            return borough\n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply the get_borough function to add the Borough column\n",
    "#df=df_search.copy()\n",
    "df_search['Borough'] = df_search.apply(lambda row: get_borough(row['Latitude'], row['Longitude'], boroughs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886481e6-d609-4767-914d-643015ae283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_percentages = {\n",
    "    'Barking and Dagenham':71 ,\n",
    "    'Barnet':70 ,\n",
    "    'Bexley':74 ,\n",
    "    'Brent':75 ,\n",
    "    'Bromley':68 ,\n",
    "    'Camden':66 ,\n",
    "    'Croydon':65 ,\n",
    "    'Ealing':76 ,\n",
    "    'Enfield':66 ,\n",
    "    'Greenwich':68 ,\n",
    "    'Hackney': 59,\n",
    "    'Hammersmith and Fulham':70 ,\n",
    "    'Haringey':59 ,\n",
    "    'Harrow':82 ,\n",
    "    'Havering':73 ,\n",
    "    'Hillingdon':79 ,\n",
    "    'Hounslow':74 ,\n",
    "    'Islington':61 ,\n",
    "    'Kensington and Chelsea':80 ,\n",
    "    'Kingston upon Thames': 72,\n",
    "    'Lambeth':63 ,\n",
    "    'Lewisham':56 ,\n",
    "    'Merton': 73,\n",
    "    'Newham':71 ,\n",
    "    'Redbridge':64 ,\n",
    "    'Richmond upon Thames':66 ,\n",
    "    'Southwark':63 ,\n",
    "    'Sutton':76 ,\n",
    "    'Tower Hamlets':72 ,\n",
    "    'Waltham Forest':57 ,\n",
    "    'Wandsworth':69 ,\n",
    "    'Westminster':78 \n",
    "}\n",
    "df_search['trust_percentage'] = df_search['Borough'].map(trust_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb13f69-b647-4c0c-ac34-6e6b1e4355fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search = df_search[df_search['Borough'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efdcd92-db0b-47d1-b73a-4414b4cdee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of low trust boroughs\n",
    "low_trust_boroughs = ['Hackney', 'Waltham', 'Lambeth','Islington']\n",
    "# Label boroughs\n",
    "df_search['trust_level'] = df_search['Borough'].apply(lambda x: 'low' if x in low_trust_boroughs else 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b341f9-62d7-44f2-8d98-803a8686beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df_search['trust_level_encoded'] = label_encoder.fit_transform(df_search['trust_level'])\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df_search[['Gender', 'Age range', 'Officer-defined ethnicity','Self-defined ethnicity','Object of search','Outcome','Outcome linked to object of search','Removal of more than just outer clothing']]\n",
    "y = df_search['trust_percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b247caf2-bd60-4e50-a8d3-3652efc1bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Create a column transformer that applies one-hot encoding to all features\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', one_hot_encoder, ['Gender', 'Age range', 'Officer-defined ethnicity','Self-defined ethnicity','Object of search','Outcome','Outcome linked to object of search','Removal of more than just outer clothing'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline that first transforms the data and then applies the random forest classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fca5b5-5b13-472e-9e64-f8c86158f0b9",
   "metadata": {},
   "source": [
    "# each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637044ce-0434-489f-8180-cd6bc77b46fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    feature  importance\n",
      "3                    Self-defined ethnicity    0.270373\n",
      "1                                 Age range    0.214038\n",
      "5                                   Outcome    0.160481\n",
      "4                          Object of search    0.147191\n",
      "2                 Officer-defined ethnicity    0.123275\n",
      "6        Outcome linked to object of search    0.068945\n",
      "0                                    Gender    0.064257\n",
      "7  Removal of more than just outer clothing    0.020385\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Extract feature importances\n",
    "# Get the feature names after one-hot encoding\n",
    "one_hot_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['onehot'].get_feature_names_out(X.columns)\n",
    "\n",
    "# Get the feature importances from the trained random forest model\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({'feature': one_hot_feature_names, 'importance': importances})\n",
    "\n",
    "# Aggregation\n",
    "original_features = X.columns\n",
    "aggregated_importances = []\n",
    "\n",
    "for feature in original_features:\n",
    "    # Select all one-hot encoded features that originated from the same original feature\n",
    "    feature_mask = feature_importance_df['feature'].str.startswith(feature)\n",
    "    # Sum the importances of these features\n",
    "    total_importance = feature_importance_df[feature_mask]['importance'].sum()\n",
    "    aggregated_importances.append({'feature': feature, 'importance': total_importance})\n",
    "\n",
    "# Create a DataFrame for aggregated importances\n",
    "aggregated_importance_df = pd.DataFrame(aggregated_importances)\n",
    "\n",
    "# Sort by importance\n",
    "aggregated_importance_df = aggregated_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(aggregated_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c7bac-cc04-438f-a777-d654e7dacde7",
   "metadata": {},
   "source": [
    "# values of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d91f3e5c-c0ca-413d-9554-8ae5da022482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              feature  importance\n",
      "4                                     Age range_25-34    0.057192\n",
      "3                                     Age range_18-24    0.055776\n",
      "26  Self-defined ethnicity_Other ethnic group - No...    0.053410\n",
      "5                                   Age range_over 34    0.040198\n",
      "2                                     Age range_10-17    0.036610\n",
      "48            Outcome linked to object of search_True    0.034778\n",
      "47           Outcome linked to object of search_False    0.034167\n",
      "33                  Object of search_Controlled drugs    0.034054\n",
      "11                    Officer-defined ethnicity_White    0.033622\n",
      "0                                       Gender_Female    0.032813\n",
      "1                                         Gender_Male    0.031444\n",
      "9                     Officer-defined ethnicity_Black    0.030357\n",
      "28  Self-defined ethnicity_White - English/Welsh/S...    0.029827\n",
      "27  Self-defined ethnicity_White - Any other White...    0.028835\n",
      "37                 Object of search_Offensive weapons    0.028118\n",
      "8                     Officer-defined ethnicity_Asian    0.028007\n",
      "41                                     Outcome_Arrest    0.027415\n",
      "40               Outcome_A no further action disposal    0.027393\n",
      "31          Object of search_Article for use in theft    0.026685\n",
      "10                    Officer-defined ethnicity_Other    0.026018\n",
      "18  Self-defined ethnicity_Black/African/Caribbean...    0.025118\n",
      "38                      Object of search_Stolen goods    0.023774\n",
      "7                                       Age range_nan    0.023432\n",
      "44                   Outcome_Khat or Cannabis warning    0.019347\n",
      "20  Self-defined ethnicity_Black/African/Caribbean...    0.016677\n",
      "25  Self-defined ethnicity_Other ethnic group - An...    0.016099\n",
      "14  Self-defined ethnicity_Asian/Asian British - B...    0.015220\n",
      "19  Self-defined ethnicity_Black/African/Caribbean...    0.014334\n",
      "35                          Object of search_Firearms    0.013948\n",
      "21  Self-defined ethnicity_Mixed/Multiple ethnic g...    0.012991\n",
      "13  Self-defined ethnicity_Asian/Asian British - A...    0.012948\n",
      "34  Object of search_Evidence of offences under th...    0.010481\n",
      "50      Removal of more than just outer clothing_True    0.010435\n",
      "24  Self-defined ethnicity_Mixed/Multiple ethnic g...    0.009967\n",
      "49     Removal of more than just outer clothing_False    0.009950\n",
      "29               Self-defined ethnicity_White - Irish    0.008102\n",
      "17  Self-defined ethnicity_Asian/Asian British - P...    0.006831\n",
      "46                  Outcome_Summons / charged by post    0.006777\n",
      "32  Object of search_Articles for use in criminal ...    0.006766\n",
      "45                Outcome_Penalty Notice for Disorder    0.006501\n",
      "30                         Self-defined ethnicity_nan    0.006044\n",
      "16  Self-defined ethnicity_Asian/Asian British - I...    0.005761\n",
      "12                      Officer-defined ethnicity_nan    0.005272\n",
      "23  Self-defined ethnicity_Mixed/Multiple ethnic g...    0.003702\n",
      "39                               Object of search_nan    0.002670\n",
      "15  Self-defined ethnicity_Asian/Asian British - C...    0.002622\n",
      "43                       Outcome_Community resolution    0.002062\n",
      "42            Outcome_Caution (simple or conditional)    0.002041\n",
      "22  Self-defined ethnicity_Mixed/Multiple ethnic g...    0.001886\n",
      "6                                  Age range_under 10    0.000831\n",
      "36                         Object of search_Fireworks    0.000694\n"
     ]
    }
   ],
   "source": [
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f5b04-4dce-488f-bcf7-83f9a3a06674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
