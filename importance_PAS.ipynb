{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78aff464",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded1608",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Data and Selecting the Right Columns\n",
    "The aim is to use as many columns as necessary at first. We also want to include the following columns: borough, age, sex, ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bca2e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\1365595396.py:1: DtypeWarning: Columns (13,14,15,16,17,18,19,20,21,22,35,177,410,411,412,415,416,417,418,419,420,421,422,423) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pas_1517 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_15_17.csv\")\n",
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\1365595396.py:2: DtypeWarning: Columns (30,31,32,98,99,100,101,102,103,104,105,125,126,127,199,200) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pas_1718 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_17_18.csv\")\n",
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\1365595396.py:3: DtypeWarning: Columns (25,26,27,28,108,109,110,112,113,137,202,203,381) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pas_1819 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_18_19.csv\")\n",
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\1365595396.py:4: DtypeWarning: Columns (41,42,43,131,132,133,213,214,435,444,451) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pas_1920 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_19_20.csv\")\n",
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\1365595396.py:5: DtypeWarning: Columns (10,11,12,13,17,24,33,34,46,47,48,49,50,51,52,53,59,60,61,64,65,66,67,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,107,108,109,110,112,113,114,115,116,119,120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pas_2021 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_20_21.csv\")\n"
     ]
    }
   ],
   "source": [
    "pas_1517 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_15_17.csv\")\n",
    "pas_1718 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_17_18.csv\")\n",
    "pas_1819 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_18_19.csv\")\n",
    "pas_1920 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_19_20.csv\")\n",
    "pas_2021 = pd.read_csv(\"pas_data_ward_level/PAS_ward_level_FY_20_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a503069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that the borough names are coherent\n",
    "pas_1517 = pas_1517.rename(columns={'BOROUGHNEIGHBOURHOOD': 'Borough'})\n",
    "pas_1718 = pas_1718.rename(columns={'BOROUGHNEIGHBOURHOOD': 'Borough'})\n",
    "pas_1819 = pas_1819.rename(columns={'BOROUGHNEIGHBOURHOOD': 'Borough'})\n",
    "pas_1920 = pas_1920.rename(columns={'BOROUGHNEIGHBOURHOOD': 'Borough'})\n",
    "\n",
    "pas_1517['Borough'] = pas_1517['Borough'].str.split(' -').str[0]\n",
    "pas_1718['Borough'] = pas_1718['Borough'].str.split(' -').str[0]\n",
    "pas_1819['Borough'] = pas_1819['Borough'].str.split(' -').str[0]\n",
    "pas_1920['Borough'] = pas_1920['Borough'].str.split(' -').str[0]\n",
    "\n",
    "pas_1517 = pas_1517.rename(columns={'NQ147r': 'Ethnicity'})\n",
    "pas_1517 = pas_1517.rename(columns={'XQ135r': 'Sex'})\n",
    "pas_1517 = pas_1517.rename(columns={'Q136r': 'Age'})\n",
    "\n",
    "pas_1718 = pas_1718.rename(columns={'NQ147r': 'Ethnicity'})\n",
    "pas_1718 = pas_1718.rename(columns={'XQ135r': 'Sex'})\n",
    "pas_1718 = pas_1718.rename(columns={'Q136r': 'Age'})\n",
    "\n",
    "pas_1819 = pas_1819.rename(columns={'NQ147r': 'Ethnicity'})\n",
    "pas_1819 = pas_1819.rename(columns={'XQ135r': 'Sex'})\n",
    "pas_1819 = pas_1819.rename(columns={'Q136r': 'Age'})\n",
    "\n",
    "pas_1920 = pas_1920.rename(columns={'NQ147r': 'Ethnicity'})\n",
    "pas_1920 = pas_1920.rename(columns={'XQ135r': 'Sex'})\n",
    "pas_1920 = pas_1920.rename(columns={'Q136r': 'Age'})\n",
    "\n",
    "pas_2021 = pas_2021.rename(columns={'ReXQ135': 'Sex'})\n",
    "pas_2021 = pas_2021.rename(columns={'ReQ136': 'Age'})\n",
    "pas_2021 = pas_2021.rename(columns={'ReNQ147': 'Ethnicity'})\n",
    "\n",
    "pas_2021['Borough'] = pas_2021['Borough'].str.replace('&', 'and', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9615eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all columns that are available in all of the datasets\n",
    "columns_1517 = set(pas_1517.columns)\n",
    "columns_1718 = set(pas_1718.columns)\n",
    "columns_1819 = set(pas_1819.columns)\n",
    "columns_1920 = set(pas_1920.columns)\n",
    "columns_2021 = set(pas_2021.columns)\n",
    "\n",
    "common_columns = list(columns_1517 & columns_1718 & columns_1819 & columns_1920 & columns_2021)\n",
    "\n",
    "filtered_columns = [\n",
    "    'A120', 'Q62TG', 'Q62A', 'Q61', 'Q62F', 'Q62C', 'Q62B',\n",
    "    'Q13', 'RQ80E', 'Q133', 'Q15', 'Q148', 'Q3L', 'Q60', 'NQ135BD']\n",
    "\n",
    "# ReXQ135 - sex (use one hot)\n",
    "# ReQ136 - age (use one hot)\n",
    "# ReNQ147 - ethnicity (use one hot)\n",
    "# Borough (use one hot, do 1 analysis without)\n",
    "\n",
    "# In age 20-21 change the word ' to ' to '-'\n",
    "# In age rest change '65 or over' to '65+'\n",
    "\n",
    "# NQ147r - ethnicity (15-20 all but the last)\n",
    "# XQ135r - sex (-\\\\-)\n",
    "# Q136 - age ()\n",
    "\n",
    "\n",
    "\n",
    "additional_columns = ['Borough', 'Age', 'Sex', 'Ethnicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a24958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the relevant columns from each dataset\n",
    "pas_1517 = pas_1517[additional_columns + filtered_columns]\n",
    "pas_1718 = pas_1718[additional_columns + filtered_columns]\n",
    "pas_1819 = pas_1819[additional_columns + filtered_columns]\n",
    "pas_1920 = pas_1920[additional_columns + filtered_columns]\n",
    "pas_2021 = pas_2021[additional_columns + filtered_columns]\n",
    "\n",
    "# Formating age\n",
    "pas_2021['Age'] = pas_2021['Age'].str.replace(\" to \", \"-\")\n",
    "pas_1517['Age'] = pas_1517['Age'].str.replace(\"65 or over\", \"65+\")\n",
    "pas_1718['Age'] = pas_1718['Age'].str.replace(\"65 or over\", \"65+\")\n",
    "pas_1819['Age'] = pas_1819['Age'].str.replace(\"65 or over\", \"65+\")\n",
    "pas_1920['Age'] = pas_1920['Age'].str.replace(\"65 or over\", \"65+\")\n",
    "\n",
    "pas_df = pd.concat([pas_1517, pas_1718, pas_1819, pas_1920, pas_2021], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7106134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 60022 entries, 1 to 76397\n",
      "Data columns (total 19 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Borough    60022 non-null  object\n",
      " 1   Age        60022 non-null  object\n",
      " 2   Sex        60022 non-null  object\n",
      " 3   Ethnicity  60022 non-null  object\n",
      " 4   A120       60022 non-null  object\n",
      " 5   Q62TG      60022 non-null  object\n",
      " 6   Q62A       60022 non-null  object\n",
      " 7   Q61        60022 non-null  object\n",
      " 8   Q62F       60022 non-null  object\n",
      " 9   Q62C       60022 non-null  object\n",
      " 10  Q62B       60022 non-null  object\n",
      " 11  Q13        60022 non-null  object\n",
      " 12  RQ80E      60022 non-null  object\n",
      " 13  Q133       60022 non-null  object\n",
      " 14  Q15        60022 non-null  object\n",
      " 15  Q148       60022 non-null  object\n",
      " 16  Q3L        60022 non-null  object\n",
      " 17  Q60        60022 non-null  object\n",
      " 18  NQ135BD    60022 non-null  object\n",
      "dtypes: object(19)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "pas_df_cleaned = pas_df.dropna()\n",
    "nan_counts = pas_df_cleaned.isna().sum()\n",
    "\n",
    "pas_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076c0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorings\n",
    "\n",
    "scoring_1 = {'Excellent': 4, 'Good': 3, 'Fair': 2, 'Poor': 1, 'Very poor': 0}\n",
    "scoring_2 = {'Definitely agree': 3, 'Tend to agree': 2, 'Tend to disagree': 1, 'Definitely disagree': 0}\n",
    "scoring_3 = {'Strongly agree': 4, 'Tend to agree': 3, 'Neither agree nor disagree': 2, 'Tend to disagree': 1, 'Strongly disagree': 0}\n",
    "scoring_4 = {'At least daily': 5, 'At least weekly': 4, 'At least fortnightly': 3, 'At least monthly': 2, 'Less often': 1, 'Never': 0}\n",
    "scoring_5 = {'Very worried': 3, 'Fairly worried': 2, 'Not very worried': 1, 'Not at all worried': 0}\n",
    "scoring_6 = {'Yes': 1, 'No': 0}\n",
    "scoring_7 = {'Very well informed': 2, 'Fairly well informed': 1, 'Not at all informed': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc2aaa0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pas_df_cleaned['Q144'] = pas_df_cleaned['Q144'].map(scoring_1)\n",
    "pas_df_cleaned.loc[:, 'A120'] = pas_df_cleaned['A120'].map(scoring_3)\n",
    "pas_df_cleaned.loc[:, 'Q62TG'] = pas_df_cleaned['Q62TG'].map(scoring_3)\n",
    "pas_df_cleaned.loc[:, 'Q62A'] = pas_df_cleaned['Q62A'].map(scoring_3)\n",
    "pas_df_cleaned.loc[:, 'Q61'] = pas_df_cleaned['Q61'].map(scoring_1)\n",
    "# pas_df_cleaned['Q131'] = pas_df_cleaned['Q131'].map(scoring_4)\n",
    "# pas_df_cleaned['NQ133A'] = pas_df_cleaned['NQ133A'].map(scoring_2)\n",
    "pas_df_cleaned.loc[:, 'Q62F'] = pas_df_cleaned['Q62F'].map(scoring_3)\n",
    "# pas_df_cleaned['Q3F'] = pas_df_cleaned['Q3F'].map(scoring_2)\n",
    "pas_df_cleaned.loc[:, 'Q62C'] = pas_df_cleaned['Q62C'].map(scoring_3)\n",
    "# pas_df_cleaned['Q1'] = pas_df_cleaned['Q1'].map(scoring_2)\n",
    "# pas_df_cleaned['Q3C'] = pas_df_cleaned['Q3C'].map(scoring_2)\n",
    "# pas_df_cleaned['Q3J'] = pas_df_cleaned['Q3J'].map(scoring_2)\n",
    "pas_df_cleaned.loc[:, 'Q62B'] = pas_df_cleaned['Q62B'].map(scoring_3)\n",
    "# pas_df_cleaned['BQ90A'] = pas_df_cleaned['BQ90A'].map(scoring_4) don't know\n",
    "pas_df_cleaned.loc[:, 'Q13'] = pas_df_cleaned['Q13'].map(scoring_5)\n",
    "pas_df_cleaned.loc[:, 'RQ80E'] = pas_df_cleaned['RQ80E'].map(scoring_6)\n",
    "pas_df_cleaned.loc[:, 'Q133'] = pas_df_cleaned['Q133'].map(scoring_7)\n",
    "pas_df_cleaned.loc[:, 'Q15'] = pas_df_cleaned['Q15'].map(scoring_5)\n",
    "pas_df_cleaned.loc[:, 'Q148'] = pas_df_cleaned['Q148'].map(scoring_6) # Disability\n",
    "# pas_df_cleaned['NQ62B'] = pas_df_cleaned['NQ62B'].map(scoring_3)\n",
    "pas_df_cleaned.loc[:, 'Q3L'] = pas_df_cleaned['Q3L'].map(scoring_2)\n",
    "# pas_df_cleaned['NQ143'] = pas_df_cleaned['NQ143'].map(scoring_4)\n",
    "pas_df_cleaned.loc[:, 'Q60'] = pas_df_cleaned['Q60'].map(scoring_1)\n",
    "pas_df_cleaned.loc[:, 'NQ135BD'] = pas_df_cleaned['NQ135BD'].map(scoring_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de84eac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Borough</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>A120</th>\n",
       "      <th>Q62TG</th>\n",
       "      <th>Q62A</th>\n",
       "      <th>Q61</th>\n",
       "      <th>Q62F</th>\n",
       "      <th>Q62C</th>\n",
       "      <th>Q62B</th>\n",
       "      <th>Q13</th>\n",
       "      <th>RQ80E</th>\n",
       "      <th>Q133</th>\n",
       "      <th>Q15</th>\n",
       "      <th>Q148</th>\n",
       "      <th>Q3L</th>\n",
       "      <th>Q60</th>\n",
       "      <th>NQ135BD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Female</td>\n",
       "      <td>White British</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>16-24</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Male</td>\n",
       "      <td>White British</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>55-64</td>\n",
       "      <td>Female</td>\n",
       "      <td>White Other</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>65+</td>\n",
       "      <td>Female</td>\n",
       "      <td>White British</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76392</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>55-64</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76393</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76394</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76396</th>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Female</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76397</th>\n",
       "      <td>Havering</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60022 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Borough    Age     Sex      Ethnicity A120 Q62TG Q62A   \n",
       "1      Kensington and Chelsea  25-34  Female  White British    3     3    2  \\\n",
       "3      Kensington and Chelsea  16-24    Male          Asian    2     3    4   \n",
       "4      Kensington and Chelsea  35-44    Male  White British    0     3    3   \n",
       "5      Kensington and Chelsea  55-64  Female    White Other    3     3    3   \n",
       "6      Kensington and Chelsea    65+  Female  White British    4     4    4   \n",
       "...                       ...    ...     ...            ...  ...   ...  ...   \n",
       "76392  Kensington and Chelsea  55-64  Female          Other    2     3    4   \n",
       "76393  Kensington and Chelsea  35-44    Male          Other    0     3    2   \n",
       "76394  Kensington and Chelsea  25-34  Female          Black    4     3    3   \n",
       "76396  Kensington and Chelsea  45-54  Female          Mixed    3     4    4   \n",
       "76397                Havering  25-34  Female          Asian    3     3    4   \n",
       "\n",
       "      Q61 Q62F Q62C Q62B Q13 RQ80E Q133 Q15 Q148 Q3L Q60 NQ135BD  \n",
       "1       3    3    3    3   2     0    1   1    0   1   3       4  \n",
       "3       3    3    4    4   0     1    1   0    0   2   3       4  \n",
       "4       3    3    3    3   1     0    0   1    0   2   3       4  \n",
       "5       3    3    3    3   1     0    0   1    1   2   3       3  \n",
       "6       3    4    4    4   1     0    0   1    0   1   3       4  \n",
       "...    ..  ...  ...  ...  ..   ...  ...  ..  ...  ..  ..     ...  \n",
       "76392   3    3    4    4   1     0    0   2    0   3   3       4  \n",
       "76393   2    3    3    3   2     0    1   1    0   3   2       3  \n",
       "76394   3    3    3    3   0     1    2   2    0   2   3       4  \n",
       "76396   4    4    3    4   1     1    0   1    0   2   3       4  \n",
       "76397   3    3    3    4   0     1    2   0    0   3   3       3  \n",
       "\n",
       "[60022 rows x 19 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pas_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf7aed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Borough      0\n",
       "Age          0\n",
       "Sex          0\n",
       "Ethnicity    0\n",
       "A120         0\n",
       "Q62TG        0\n",
       "Q62A         0\n",
       "Q61          0\n",
       "Q62F         0\n",
       "Q62C         0\n",
       "Q62B         0\n",
       "Q13          0\n",
       "RQ80E        0\n",
       "Q133         0\n",
       "Q15          0\n",
       "Q148         0\n",
       "Q3L          0\n",
       "Q60          0\n",
       "NQ135BD      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_counts = pas_df_cleaned.isna().sum()\n",
    "nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601a9dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\2309912597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pas_df_cleaned.drop(['Age', 'Sex', 'Ethnicity', 'Borough'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Doing one hot encoding \n",
    "\n",
    "age_dummies = pd.get_dummies(pas_df_cleaned['Age'], prefix='Age', dtype=int)\n",
    "sex_dummies = pd.get_dummies(pas_df_cleaned['Sex'], prefix='Sex', dtype=int)\n",
    "ethnicity_dummies = pd.get_dummies(pas_df_cleaned['Ethnicity'], prefix='Ethnicity', dtype=int)\n",
    "\n",
    "pas_df_cleaned.drop(['Age', 'Sex', 'Ethnicity', 'Borough'], axis=1, inplace=True)\n",
    "\n",
    "df_pas = pd.concat([pas_df_cleaned, age_dummies, sex_dummies, ethnicity_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa09435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A120</th>\n",
       "      <th>Q62TG</th>\n",
       "      <th>Q62A</th>\n",
       "      <th>Q61</th>\n",
       "      <th>Q62F</th>\n",
       "      <th>Q62C</th>\n",
       "      <th>Q62B</th>\n",
       "      <th>Q13</th>\n",
       "      <th>RQ80E</th>\n",
       "      <th>Q133</th>\n",
       "      <th>...</th>\n",
       "      <th>Sex_Female</th>\n",
       "      <th>Sex_Male</th>\n",
       "      <th>Sex_Other</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black</th>\n",
       "      <th>Ethnicity_Mixed</th>\n",
       "      <th>Ethnicity_Other</th>\n",
       "      <th>Ethnicity_Refused</th>\n",
       "      <th>Ethnicity_White British</th>\n",
       "      <th>Ethnicity_White Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76392</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76393</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76394</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76396</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76397</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60022 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A120 Q62TG Q62A Q61 Q62F Q62C Q62B Q13 RQ80E Q133  ... Sex_Female   \n",
       "1        3     3    2   3    3    3    3   2     0    1  ...          1  \\\n",
       "3        2     3    4   3    3    4    4   0     1    1  ...          0   \n",
       "4        0     3    3   3    3    3    3   1     0    0  ...          0   \n",
       "5        3     3    3   3    3    3    3   1     0    0  ...          1   \n",
       "6        4     4    4   3    4    4    4   1     0    0  ...          1   \n",
       "...    ...   ...  ...  ..  ...  ...  ...  ..   ...  ...  ...        ...   \n",
       "76392    2     3    4   3    3    4    4   1     0    0  ...          1   \n",
       "76393    0     3    2   2    3    3    3   2     0    1  ...          0   \n",
       "76394    4     3    3   3    3    3    3   0     1    2  ...          1   \n",
       "76396    3     4    4   4    4    3    4   1     1    0  ...          1   \n",
       "76397    3     3    4   3    3    3    4   0     1    2  ...          1   \n",
       "\n",
       "      Sex_Male Sex_Other Ethnicity_Asian Ethnicity_Black  Ethnicity_Mixed   \n",
       "1            0         0               0               0                0  \\\n",
       "3            1         0               1               0                0   \n",
       "4            1         0               0               0                0   \n",
       "5            0         0               0               0                0   \n",
       "6            0         0               0               0                0   \n",
       "...        ...       ...             ...             ...              ...   \n",
       "76392        0         0               0               0                0   \n",
       "76393        1         0               0               0                0   \n",
       "76394        0         0               0               1                0   \n",
       "76396        0         0               0               0                1   \n",
       "76397        0         0               1               0                0   \n",
       "\n",
       "       Ethnicity_Other  Ethnicity_Refused  Ethnicity_White British   \n",
       "1                    0                  0                        1  \\\n",
       "3                    0                  0                        0   \n",
       "4                    0                  0                        1   \n",
       "5                    0                  0                        0   \n",
       "6                    0                  0                        1   \n",
       "...                ...                ...                      ...   \n",
       "76392                1                  0                        0   \n",
       "76393                1                  0                        0   \n",
       "76394                0                  0                        0   \n",
       "76396                0                  0                        0   \n",
       "76397                0                  0                        0   \n",
       "\n",
       "       Ethnicity_White Other  \n",
       "1                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "5                          1  \n",
       "6                          0  \n",
       "...                      ...  \n",
       "76392                      0  \n",
       "76393                      0  \n",
       "76394                      0  \n",
       "76396                      0  \n",
       "76397                      0  \n",
       "\n",
       "[60022 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3446a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified target distribution:\n",
      "[ 3196 51787]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\779449902.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pas['NQ135BD'] = df_pas['NQ135BD'].apply(lambda x: 0 if x in [0, 1] else 1)\n"
     ]
    }
   ],
   "source": [
    "df_pas = df_pas[df_pas['NQ135BD'] != 2]  # Remove rows where the value is 2\n",
    "df_pas['NQ135BD'] = df_pas['NQ135BD'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "\n",
    "# Check the distribution of the modified target variable\n",
    "print('Modified target distribution:')\n",
    "print(np.bincount(df_pas['NQ135BD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fccc713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X = df_pas.drop('NQ135BD', axis=1)\n",
    "y = df_pas['NQ135BD']\n",
    "\n",
    "# I researched how to split the data into 3 with this link: https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert the target variable to integer type\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "y_test = y_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10fd2e",
   "metadata": {},
   "source": [
    "### For later, the dataset is not very balanced, take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e67362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set target distribution\n",
      "[ 1962 31027]\n"
     ]
    }
   ],
   "source": [
    "print('Training set target distribution')\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d08f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_pas.corr()\n",
    "threshold = 0.70\n",
    "high_corr_var = np.where(np.abs(correlation_matrix) > threshold)\n",
    "high_corr_var = [(correlation_matrix.index[x], correlation_matrix.columns[y]) \n",
    "                 for x, y in zip(*high_corr_var) \n",
    "                 if x != y and x < y]\n",
    "\n",
    "features_to_drop = set()\n",
    "\n",
    "for (feature1, feature2) in high_corr_var:\n",
    "    features_to_drop.add(feature2)\n",
    "\n",
    "X_train_reduced = X_train.drop(columns=list(features_to_drop))\n",
    "X_val_reduced = X_val.drop(columns=list(features_to_drop))\n",
    "X_test_reduced = X_test.drop(columns=list(features_to_drop))\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
    "X_val_reduced_scaled = scaler.transform(X_val_reduced)\n",
    "X_test_reduced_scaled = scaler.transform(X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab963a",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3edb47c",
   "metadata": {},
   "source": [
    "### Grid Search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac6c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression() \n",
    "\n",
    "# param_grid = [\n",
    "#     {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2'], 'multi_class': ['ovr', 'multinomial'], 'solver': ['lbfgs', 'saga']},\n",
    "#     {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1'], 'multi_class': ['ovr'], 'solver': ['liblinear', 'saga']}\n",
    "# ]\n",
    "\n",
    "# # Set up the GridSearchCV\n",
    "# grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train_reduced_scaled, y_train)\n",
    "\n",
    "# # Get the best parameters and the best model\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# # Evaluate the best model on the validation set\n",
    "# y_val_pred = best_model.predict(X_val_reduced_scaled)\n",
    "# val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "# print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# print(\"Classification report on validation set:\")\n",
    "# print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "# y_test_pred = best_model.predict(X_test_reduced_scaled)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# print(\"Classification report on test set:\")\n",
    "# print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# 'C': 0.01, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'saga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358566c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.963808311357643\n",
      "Classification report on validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.48      0.58       583\n",
      "           1       0.97      0.99      0.98     10414\n",
      "\n",
      "    accuracy                           0.96     10997\n",
      "   macro avg       0.86      0.73      0.78     10997\n",
      "weighted avg       0.96      0.96      0.96     10997\n",
      "\n",
      "Test accuracy: 0.9595344184777667\n",
      "Classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.46      0.57       651\n",
      "           1       0.97      0.99      0.98     10346\n",
      "\n",
      "    accuracy                           0.96     10997\n",
      "   macro avg       0.87      0.72      0.78     10997\n",
      "weighted avg       0.95      0.96      0.95     10997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.01, multi_class='ovr', penalty='l2', solver='saga', max_iter=100000000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train_reduced_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = logreg.predict(X_val_reduced_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "print(\"Classification report on validation set:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = logreg.predict(X_test_reduced_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "print(\"Classification report on test set:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6043143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for test set:\n",
      "[[  298   353]\n",
      " [   92 10254]]\n",
      "Coefficients of the logistic regression model:\n",
      "[[ 0.29188457  0.31698572  0.37019105  0.60474368  0.43593847  0.30381905\n",
      "  -0.04070985 -0.03749153  0.0647547   0.05758224 -0.12832752  0.07490014\n",
      "   0.35628333 -0.01935241 -0.04905314  0.01794811 -0.00119779  0.00746878\n",
      "   0.04066257 -0.03908132  0.07264939 -0.03801168  0.07202208 -0.24146007\n",
      "  -0.09770877  0.00981411 -0.06182092  0.07669239  0.07217403]]\n",
      "                    Feature  Coefficient\n",
      "3                       Q61     0.604744\n",
      "4                      Q62C     0.435938\n",
      "2                      Q62A     0.370191\n",
      "12                      Q60     0.356283\n",
      "1                     Q62TG     0.316986\n",
      "5                      Q62B     0.303819\n",
      "0                      A120     0.291885\n",
      "23          Ethnicity_Black    -0.241460\n",
      "10                     Q148    -0.128328\n",
      "24          Ethnicity_Mixed    -0.097709\n",
      "27  Ethnicity_White British     0.076692\n",
      "11                      Q3L     0.074900\n",
      "20               Sex_Female     0.072649\n",
      "28    Ethnicity_White Other     0.072174\n",
      "22          Ethnicity_Asian     0.072022\n",
      "8                      Q133     0.064755\n",
      "26        Ethnicity_Refused    -0.061821\n",
      "9                       Q15     0.057582\n",
      "14                Age_25-34    -0.049053\n",
      "6                       Q13    -0.040710\n",
      "18                  Age_65+     0.040663\n",
      "19              Age_Refused    -0.039081\n",
      "21                Sex_Other    -0.038012\n",
      "7                     RQ80E    -0.037492\n",
      "13                Age_16-24    -0.019352\n",
      "15                Age_35-44     0.017948\n",
      "25          Ethnicity_Other     0.009814\n",
      "17                Age_55-64     0.007469\n",
      "16                Age_45-54    -0.001198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Test set Confusion Matrix\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion matrix for test set:\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "# Coefficients\n",
    "coefficients = logreg.coef_\n",
    "print(\"Coefficients of the logistic regression model:\")\n",
    "print(coefficients)\n",
    "\n",
    "# Extract the coefficients of the logistic regression model\n",
    "feature_names = X_train_reduced.columns\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': logreg.coef_[0]  # Since we have binary classification, there is only one set of coefficients\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of the coefficients for better readability\n",
    "coefficients_df['Absolute Coefficient'] = coefficients_df['Coefficient'].abs()\n",
    "coefficients_df = coefficients_df.sort_values(by='Absolute Coefficient', ascending=False).drop('Absolute Coefficient', axis=1)\n",
    "\n",
    "print(coefficients_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f875c",
   "metadata": {},
   "source": [
    "### Statsmodels for statistical signifficance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b335080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.116752\n",
      "         Iterations 351\n"
     ]
    }
   ],
   "source": [
    "X_train_const = sm.add_constant(X_train_reduced_scaled)\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit(maxiter=1000)\n",
    "summary_table = result.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40921d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistically Signifficant coefficients\n",
      "        Coef.  Std.Err.          z         P>|z|    [0.025    0.975]\n",
      "x1   0.311596  0.027387  11.377667  5.402594e-30  0.257919  0.365272\n",
      "x2   0.322602  0.031739  10.164226  2.863879e-24  0.260395  0.384809\n",
      "x3   0.387871  0.031819  12.189840  3.521129e-34  0.325507  0.450236\n",
      "x4   0.650429  0.032773  19.846237  1.187685e-87  0.586194  0.714664\n",
      "x5   0.454698  0.030131  15.090546  1.868893e-51  0.395642  0.513754\n",
      "x6   0.305917  0.028353  10.789748  3.848420e-27  0.250347  0.361487\n",
      "x10  0.072294  0.035403   2.042048  4.114673e-02  0.002906  0.141683\n",
      "x11 -0.142583  0.031069  -4.589222  4.449011e-06 -0.203478 -0.081689\n",
      "x12  0.078641  0.029760   2.642486  8.229991e-03  0.020312  0.136970\n",
      "x13  0.361411  0.035669  10.132459  3.965501e-24  0.291501  0.431320\n",
      "x21  0.082773  0.031826   2.600790  9.300940e-03  0.020395  0.145151\n",
      "x22 -0.039434  0.018618  -2.118029  3.417261e-02 -0.075925 -0.002943\n"
     ]
    }
   ],
   "source": [
    "significant_coefficients = summary_table[summary_table['P>|z|'] < 0.05]\n",
    "\n",
    "print('Statistically Signifficant coefficients')\n",
    "print(significant_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4701790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistically Significant Coefficients:\n",
      "       Feature  Coefficient_statsmodels P-Value  Coefficient_scikit\n",
      "0          Q61                 0.650429  0.0000            0.604744\n",
      "1         Q62C                 0.454698  0.0000            0.435938\n",
      "2         Q62A                 0.387871  0.0000            0.370191\n",
      "3          Q60                 0.361411  0.0000            0.356283\n",
      "4        Q62TG                 0.322602  0.0000            0.316986\n",
      "5         A120                 0.311596  0.0000            0.291885\n",
      "6         Q62B                 0.305917  0.0000            0.303819\n",
      "7         Q148                -0.142583  0.0000           -0.128328\n",
      "8   Sex_Female                 0.082773  0.0093            0.072649\n",
      "9          Q3L                 0.078641  0.0082            0.074900\n",
      "10         Q15                 0.072294  0.0411            0.057582\n",
      "11   Sex_Other                -0.039434  0.0342           -0.038012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20220900\\AppData\\Local\\Temp\\ipykernel_27440\\2085553541.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  significant_coefficients_df['Absolute Coefficient'] = significant_coefficients_df['Coefficient_statsmodels'].abs()\n"
     ]
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = X_train_reduced.columns\n",
    "feature_names = ['const'] + list(feature_names)\n",
    "\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient_statsmodels': result.params,\n",
    "    'P-Value': result.pvalues\n",
    "})\n",
    "\n",
    "# Only signifficant under 95% CI\n",
    "significant_coefficients_df = coefficients_df[coefficients_df['P-Value'] < 0.05]\n",
    "significant_coefficients_df['Absolute Coefficient'] = significant_coefficients_df['Coefficient_statsmodels'].abs()\n",
    "significant_coefficients_df = significant_coefficients_df.sort_values(by='Absolute Coefficient', ascending=False).drop('Absolute Coefficient', axis=1)\n",
    "\n",
    "# Get coefficients from the more advanced model and add them to the stats models summary table\n",
    "logreg_coefficients = logreg.coef_[0]\n",
    "significant_coefficients_df = significant_coefficients_df.reset_index(drop=True)\n",
    "logreg_coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names[1:],\n",
    "    'Coefficient_scikit': logreg_coefficients\n",
    "})\n",
    "merged_df = pd.merge(significant_coefficients_df, logreg_coefficients_df, on='Feature', how='left')\n",
    "merged_df['P-Value'] = merged_df['P-Value'].apply(lambda x: f'{x:.4f}')\n",
    "#\n",
    "print('Statistically Significant Coefficients:')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067bc3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistically Significant Coefficients with Odds Ratios:\n",
      "       Feature  Coefficient_statsmodels P-Value  Coefficient_scikit   \n",
      "0          Q61                 0.650429  0.0000            0.604744  \\\n",
      "1         Q62C                 0.454698  0.0000            0.435938   \n",
      "2         Q62A                 0.387871  0.0000            0.370191   \n",
      "3          Q60                 0.361411  0.0000            0.356283   \n",
      "4        Q62TG                 0.322602  0.0000            0.316986   \n",
      "5         A120                 0.311596  0.0000            0.291885   \n",
      "6         Q62B                 0.305917  0.0000            0.303819   \n",
      "7         Q148                -0.142583  0.0000           -0.128328   \n",
      "8   Sex_Female                 0.082773  0.0093            0.072649   \n",
      "9          Q3L                 0.078641  0.0082            0.074900   \n",
      "10         Q15                 0.072294  0.0411            0.057582   \n",
      "11   Sex_Other                -0.039434  0.0342           -0.038012   \n",
      "\n",
      "    Odds_Ratio_statsmodels  Odds_Ratio_scikit  \n",
      "0                 1.916362           1.830783  \n",
      "1                 1.575697           1.546414  \n",
      "2                 1.473840           1.448011  \n",
      "3                 1.435353           1.428012  \n",
      "4                 1.380715           1.372983  \n",
      "5                 1.365602           1.338948  \n",
      "6                 1.357870           1.355024  \n",
      "7                 0.867115           0.879565  \n",
      "8                 1.086295           1.075353  \n",
      "9                 1.081816           1.077777  \n",
      "10                1.074972           1.059272  \n",
      "11                0.961333           0.962702  \n"
     ]
    }
   ],
   "source": [
    "# Calculate odds ratios\n",
    "merged_df['Odds_Ratio_statsmodels'] = np.exp(merged_df['Coefficient_statsmodels'])\n",
    "merged_df['Odds_Ratio_scikit'] = np.exp(merged_df['Coefficient_scikit'])\n",
    "\n",
    "\n",
    "print('Statistically Significant Coefficients with Odds Ratios:')\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653beeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
